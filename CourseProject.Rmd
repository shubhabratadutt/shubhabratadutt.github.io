---
title: "Practical Machine learning Course Project"
author: "shubhabratadutt"
date: "Sunday, May 24, 2015"
output: html_document
---


##Objective##

The objective of this project is to develop a predictor for physical exercise effectiveness given 
the sensor data captured from wearable devices. The training and test data for this project 
has been sources from http://groupware.les.inf.puc-rio.br/har


##Data Loading and Preparation##

The following steps are executing for loading and preparing the 
data sets:
1. Training and Test data sets are loaded
2. Covariates with near zero variability are removed as they do not contribute
   much to the prediction.
3. Covariates with more than 80% mssing values are also removed for the same reason.
```{r, cache=TRUE}
library(caret)
library(ggplot2)

#Loading the training and test data sets
trainingSet <- read.csv("pml-training.csv")
testSet <- read.csv("pml-testing.csv")

#Remove Near Zero Covariates from the training set
nearZeroVars <- nearZeroVar(trainingSet, saveMetrics = T)
trainingSet <- trainingSet[, !nearZeroVars$nzv]

#Remove columns with more than 80% of mising values
colsToRemove <- sapply(colnames(trainingSet), function(x) if(sum(is.na(trainingSet[, x])) > 0.8*nrow(trainingSet)){return(T)}else{return(F)})
trainingSet <- trainingSet[, !colsToRemove]
```


##Model Selection##

We will try Boosting and Random Forest models.
The better model will be selected based on the accuracy.
The trining data set is partitioned into two parts, one for training
and the other for testing.

```{r, cache=TRUE}
set.seed(123)
inTrain <- createDataPartition(y=trainingSet$classe, p=0.6, list=FALSE)
myTraining <- trainingSet[inTrain, ]
myTesting <- trainingSet[-inTrain, ]
```

###Boosting with 10 fold cross validation###
```{r, cache=TRUE}
boostFit <- train(classe ~ ., method = "gbm", data = myTraining, verbose = F, trControl = trainControl(method = "cv", number = 10))
predictionsMyTraining <- predict(boostFit, newdata=myTraining)
cm1 <- confusionMatrix(predictionsMyTraining, myTraining$classe)
cm1
#In sample accuracy
cm1$overall[1]

#Applying the model to the testing subset of the original 
#training set
predictionsMyTesting <- predict(boostFit, newdata=myTesting)
cm2 <- confusionMatrix(predictionsMyTesting, myTesting$classe)
cm2
#Out of sample accuracy
cm2$overall[1]

```

The in sample as well as the out of sample accuracy, of the predicion model using Boosting, is 1.00!

###Random forest with 10 fold cross validation###
```{r, cache=TRUE}
rfFit <- train(classe ~ ., method = "rf", data = myTraining, verbose = F, trControl = trainControl(method = "cv", number = 10))
predictionsMyTrainingRf <- predict(rfFit, newdata=myTraining)
cm3 <- confusionMatrix(predictionsMyTrainingRf, myTraining$classe)
cm3
#In sample accuracy
cm3$overall[1]

#Applying the model to the testing subset of the original 
#training set
predictionsMyTestingRf <- predict(rfFit, newdata=myTesting)
cm4 <- confusionMatrix(predictionsMyTestingRf, myTesting$classe)
cm4
#Out of sample accuracy
cm2$overall[1]
````

The in sample as well as the out of sample accuracy, of the prediction model using Random forest, is 1.0!

###Predicting with the selected model###
At this point either of the two models may be used since they are tied on accuracy of prediction.
We chose to proceed with the model based on Random Forest.

```{r}
#Predicting the outcome, classe, for the 20 observations
#in the orignally provided test data set.
predictionsTest <- predict(rfFit, newdata=testSet)
predictionsTest
```

###Generating the answer files for submission###
```{r}
pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictionsTest)
```

